{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKiueR0QDKdY"
      },
      "source": [
        "# **Naive Bayes Classifier**\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "77a59b094ff9a1a7a31a4e8c51921941f8ce383f",
        "id": "WQp4bwygDKdg"
      },
      "source": [
        "**What is Naive Bayes algorithm?**\n",
        "\n",
        "It is a classification method built on the Bayes Theorem and predicated on the idea of predictor independence. A Naive Bayes classifier, to put it simply, believes that the presence of one feature in a class has nothing to do with the presence of any other feature.\n",
        "\n",
        "A fruit might be categorized as an apple, for instance, if it is red, rounded, and around 3 inches in diameter. Even if these characteristics depend on one another or on the presence of other characteristics, each of these traits separately increases the likelihood that this fruit is an apple, which is why it is called \"Naive.\"\n",
        "\n",
        "> Naive Bayes model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods.\n",
        "\n",
        "Bayes theorem provides a way of calculating posterior probability P(c|x) - *(read as Probability of **c** given **x**)*,  from P(c), P(x) and P(x|c). Look at the equation below:\n",
        ">\n",
        "> $$\\mathbf{P} \\left({x \\mid c} \\right) = \\frac{\\mathbf{P} \\left ({c \\mid x} \\right) \\mathbf{P} \\left({c} \\right)}{\\mathbf{P} \\left( {x} \\right)}$$\n",
        "\n",
        "where,\n",
        "\n",
        "* *x is set of features*\n",
        "* *c is set of classes*\n",
        "* P(c|x) is the posterior probability of class (c, target) given predictor (x, attributes).\n",
        "* P(c) is the prior probability of class **c**.\n",
        "* P(x|c) is the observation density or likelihood which is the probability of predictor(the query  **x**) given class.\n",
        "* P(x) is the prior probability of predictor **x**, and it is also called as Evidence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c0ac90e7494324245b5c8088281a784e03690ac9",
        "id": "b-MW9z2jDKdi"
      },
      "source": [
        "**Why should we use Naive Bayes ?**\n",
        "\n",
        "* As stated above, It is **_easy_** to build and is particularly useful for **_very large data sets_**.\n",
        "* It is **extremely fast** for both training and prediction.\n",
        "* It provide straightforward probabilistic prediction.\n",
        "* It is often very easily interpretable.\n",
        "* It has very few (if any) tunable parameters.\n",
        "* It perform well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption).\n",
        "\n",
        "**Disadvantages of Naïve Bayes Classifier:**\n",
        "\n",
        "Naive Bayes assumes that all features are independent or unrelated, so it cannot learn the relationship between features.\n",
        "\n",
        "**Applications of Naïve Bayes Classifier:**\n",
        "\n",
        "It is used for Credit Scoring.\n",
        "\n",
        "It is used in medical data classification.\n",
        "\n",
        "It can be used in real-time predictions because Naïve Bayes Classifier is an eager learner.\n",
        "\n",
        "It is used in Text classification such as Spam filtering and Sentiment analysis.\n",
        "\n",
        "**Types of Naïve Bayes Model:**\n",
        "\n",
        "There are three different forms of naive bayes models, which are listed below:\n",
        "\n",
        "**Gaussian:** The Gaussian model presupposes that features are distributed normally. This indicates that the model thinks that predictor values are samples from the Gaussian distribution if they take continuous values rather than discrete ones.\n",
        "\n",
        "**Multinomial Naive Bayes:** When the data is multinomially distributed, the Multinomial Naive Bayes classifier is employed. It is mostly used for document classification issues, indicating which category a specific document belongs to.\n",
        "Word frequency is used by the classifier as a predictor.\n",
        "\n",
        "**Bernoulli classifier:** In contrast to the Multinomial classifier, the Bernoulli classifier uses independent Boolean values as predictor variables. such as determining whether a word is used or not in a document.\n",
        "\n",
        "**Steps to implement:**\n",
        "\n",
        "Data Pre-processing step\n",
        "\n",
        "Fitting Naive Bayes to the Training set\n",
        "\n",
        "Predicting the test result\n",
        "\n",
        "Test accuracy of the result(Creation of Confusion matrix)\n",
        "\n",
        "Visualizing the test set result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "NFcBZofCflTx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sys import path\n",
        "import re\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GApzKNT9flT3"
      },
      "source": [
        "### read file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "YOuK2peHflT9"
      },
      "outputs": [],
      "source": [
        "TRAIN_DATA= \"/content/train_data.csv\"\n",
        "TEST_DATA = \"/content/test_data.csv\"\n",
        "DEV =\"/content/sample_submission.csv\"\n",
        "data_train = pd.read_csv(TRAIN_DATA)\n",
        "data_test  = pd.read_csv(TEST_DATA)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df= pd.concat([data_train, data_test], axis=0)"
      ],
      "metadata": {
        "id": "zj2cKCGAhblA"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk-rxqLgflT_"
      },
      "source": [
        "## a.Divide the dataset as train, development and test. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "iUSEDllMflUA",
        "outputId": "84a132d7-7fea-40e8-f342-f9c779e9ff2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 60115\n",
            "Dev dataset size: 5\n",
            "Test dataset size: 15029\n"
          ]
        }
      ],
      "source": [
        "# Split dataset to k folds\n",
        "def crossValidationSplit(data, k_folds):\n",
        "    data_split = list()\n",
        "    data_copy = list(data)\n",
        "    size = int(len(data) / k_folds)\n",
        "    for _ in range(k_folds):\n",
        "        fold = list()\n",
        "        while len(fold) < size:\n",
        "            k = random.randrange(len(data_copy))\n",
        "            fold.append(data_copy.pop(k))\n",
        "        data_split.append(fold)\n",
        "    return data_split\n",
        "\n",
        "def splitDataToTrainAndDev(dataset, k_folds):\n",
        "    folds = crossValidationSplit(dataset, k_folds)\n",
        "    train_set, dev_set = [], []\n",
        "    for fold in folds:\n",
        "        train_set = list(folds)\n",
        "        train_set.remove(fold)\n",
        "        train_set = sum(train_set, [])\n",
        "        dev_set = list()\n",
        "        for row in fold:\n",
        "                row_copy = list(row)\n",
        "                dev_set.append(row_copy)\n",
        "        break\n",
        "    return train_set, dev_set\n",
        "\n",
        "data_train = pd.read_csv(TRAIN_DATA)\n",
        "\n",
        "dev = pd.read_csv(DEV)\n",
        "\n",
        "data_test  = pd.read_csv(TEST_DATA)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('Train dataset size: ' + str(len(data_train)))\n",
        "print('Dev dataset size: ' + str(len(dev)))\n",
        "print('Test dataset size: ' + str(len(data_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyyD-FyDflUD"
      },
      "source": [
        "## b.Build a vocabulary as list. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQg0DU2JflUE"
      },
      "source": [
        "\\[‘the’ ‘I’ ‘happy’ … \\] \n",
        "\n",
        "You may omit rare words for example if the occurrence is less than five times \n",
        "\n",
        "A reverse index as the key value might be handy \n",
        "\n",
        "{“the”: 0, “I”:1, “happy”:2 , … }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "OdP74kGeflUG",
        "outputId": "ba702423-aa58-4b0d-aaa1-5ef6da46dcf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ],
      "source": [
        "def SegmentLineToWords(string):\n",
        "    string=string.replace('<br />', '')\n",
        "    return set([x.lower() for x in re.split(r'[\\s|,|;|.|/|\\[|\\]|;|\\!|?|\\'|\\\\|\\)|\\(|\\\"|@|&|#|-|*|%|>|<|^|-]\\s*',string.strip()) if x])\n",
        "\n",
        "def buildVocabularyList(dataset):\n",
        "    dict_list = {} #{'word':[merged_df, dev_count]}\n",
        "    for row in dataset:\n",
        "        words = set() #Words that appear multiple times in the same comment are counted only once\n",
        "        words = words.union(SegmentLineToWords(str(row[0])))\n",
        "        for word in words:\n",
        "            if word not in dict_list:\n",
        "                dict_list[word] = [0,0]\n",
        "            if row[1] == -1:\n",
        "                dict_list[word][0] += 1\n",
        "            else:\n",
        "                dict_list[word][1] += 1\n",
        "    for word in list(dict_list.keys()):\n",
        "        if dict_list[word][0] + dict_list[word][1]<5:\n",
        "            del dict_list[word]\n",
        "    return dict_list\n",
        "train_dict = buildVocabularyList(data_train)\n",
        "train_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2SGP8EbflUJ"
      },
      "source": [
        "## c.Calculate the following probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0RfX0sbflUJ"
      },
      "source": [
        "Probability of the occurrence\n",
        "\n",
        "P\\[“the”\\] = num of documents containing ‘the’ / num of all documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "rZvCpE6qflUL",
        "outputId": "9a914578-25ee-4905-e612-b7ff124f0bea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P[“the”] = 0\n"
          ]
        }
      ],
      "source": [
        "def getProbabilityOfOccurrence(word):\n",
        "    if word not in train_dict:\n",
        "        return 0\n",
        "    else: \n",
        "        return (train_dict[word][0] + train_dict[word][1])/(len(data_train))\n",
        "print(\"P[“the”] = \" + str(getProbabilityOfOccurrence(\"the\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqliZjBHflUM"
      },
      "source": [
        "Conditional probability based on the sentiment\n",
        "\n",
        "P\\[“the” | Positive\\]  = # of positive documents containing “the” / num of all positive review documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "Pg6jQUfDflUM",
        "outputId": "84029759-d088-4485-d3ae-2bfb61e4397e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P[“the” | data_train] = 0\n"
          ]
        }
      ],
      "source": [
        "def getPosConditionalProbability(word):\n",
        "    if word not in train_dict:\n",
        "        return 0\n",
        "    else:\n",
        "        return train_dict[word][1]/(len(data_train))\n",
        "\n",
        "print(\"P[“the” | data_train] = \" + str(getPosConditionalProbability(\"the\")))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RByIW7xxflUN"
      },
      "source": [
        "## d.Calculate accuracy using dev dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "hFtGBrqkflUO"
      },
      "outputs": [],
      "source": [
        "def predict(review, smoothing_flag):\n",
        "    words = set()\n",
        "    words = words.union(SegmentLineToWords(review))\n",
        "    data_train_probability = 1\n",
        "    \n",
        "    for word in words:\n",
        "        if smoothing_flag == 1:\n",
        "            data_train_probability *= getPosConditionalProbabilityUsingSmoothing(word)\n",
        "        \n",
        "            \n",
        "        else:\n",
        "            #print(word)\n",
        "            #print(\"getdatatrainConditionalProbability: \" + str(getPosConditionalProbability(word)))\n",
        "         \n",
        "            data_train_probability *= getdatatrainConditionalProbability(word)\n",
        "            \n",
        "    #print(\"data_train_probability: \" + str(data_train_probability))\n",
        "    \n",
        "   \n",
        "\n",
        "def accuracy_metric(test_dataset, smoothing_flag):\n",
        "    correct = 0\n",
        "    for row in test_dataset:\n",
        "        #print( predict(str(row[0]), smoothing_flag))\n",
        "        #print(row[1])\n",
        "        if row[1] == predict(str(row[0]), smoothing_flag):\n",
        "            correct += 1\n",
        "    return correct / float(len(test_dataset)) * 100.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "8a46pcO1flUP",
        "outputId": "7081a809-10a4-4546-d4d1-704d1b35e2c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: %.3f%%\n"
          ]
        }
      ],
      "source": [
        "#train_dict\n",
        "print('Accuracy: %.3f%%' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzkPQ1_KflUQ"
      },
      "source": [
        "### Conduct five fold cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "WWyBPqyjflUQ",
        "outputId": "19c0583c-a250-49c8-d12f-9a45c6ef648a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores: [<function accuracy_metric at 0x7f9f6cfeab00>, <function accuracy_metric at 0x7f9f6cfeab00>, <function accuracy_metric at 0x7f9f6cfeab00>, <function accuracy_metric at 0x7f9f6cfeab00>, <function accuracy_metric at 0x7f9f6cfeab00>]\n",
            "Mean Accuracy: %.3f%%\n"
          ]
        }
      ],
      "source": [
        "def evaluate_algorithm(pos_dataset, neg_dataset, k_folds, smoothing_flag):\n",
        "    data_train_folds = crossValidationSplit(pos_dataset, k_folds)\n",
        "    scores = list()\n",
        "    for i in range(0,len(data_train_folds)):\n",
        "        data_train = list(data_train_folds)\n",
        "       \n",
        "        \n",
        "        data_train.remove(data_train_folds[i])\n",
        "       \n",
        "        \n",
        "        data_train = sum(data_train, [])\n",
        "        \n",
        "        \n",
        "        dev = list()\n",
        "        \n",
        "       \n",
        "        \n",
        "        dev = dev\n",
        "        accuracy = accuracy_metric\n",
        "        scores.append(accuracy)\n",
        "    print('Scores: %s' % scores)\n",
        "    print('Mean Accuracy: %.3f%%' )\n",
        "smoothing_flag = 0\n",
        "evaluate_algorithm(dataset_pos, dataset_neg, 5, smoothing_flag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvlUlWx8flUR"
      },
      "source": [
        "## e.Do following experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y5AYQK4flUR"
      },
      "source": [
        "### Compare the effect of Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "S147CRmFflUS"
      },
      "outputs": [],
      "source": [
        "lambda_value = 1\n",
        "def getPosConditionalProbabilityUsingSmoothing(word):\n",
        "    if word not in train_dict:\n",
        "        return lambda_value/(2*lambda_value+len(data_train))\n",
        "    else:\n",
        "        return (lambda_value + train_dict[word][1])/(2*lambda_value+len(data_train))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "HFInhfUpflUS",
        "outputId": "d73ef03a-661e-4639-e177-4eebea5eb611",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy not using Smoothing: %.3f%%\n",
            "Accuracy by using Smoothing: %.3f%%\n"
          ]
        }
      ],
      "source": [
        "smoothing_flag = 1\n",
        "\n",
        "print('Accuracy not using Smoothing: %.3f%%' )\n",
        "print('Accuracy by using Smoothing: %.3f%%'  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysW_t-WLflUU"
      },
      "source": [
        "### As we can see,if only the training set and formula are used to count the data obtained, the performance is not good. So the in the follows i will use the dev and algorithm to get top 10 word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "5N5gCh59flUU"
      },
      "outputs": [],
      "source": [
        "def getDevPredictsList():\n",
        "    predicts_list = {}\n",
        "    correct = 0\n",
        "    for row in dev:\n",
        "        if row[1] == predict(str(row[0]), 1):\n",
        "            words = set()\n",
        "            words = words.union(SegmentLineToWords(str(row[0])))\n",
        "            for word in words:\n",
        "                if word not in predicts_list:\n",
        "                    predicts_list[word] = [0,0]\n",
        "                if row[1] == -1:\n",
        "                    predicts_list[word][0] += 1\n",
        "                else:\n",
        "                    predicts_list[word][1] += 1\n",
        "    return predicts_list\n",
        "predicts_list = getDevPredictsList()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "CnZd5QG-flUV",
        "outputId": "6220ba9b-badc-4486-dcb3-e1d10cc01550",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 words that predicts  using dev data :\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def getTop10UsingDev(label):\n",
        "    positive_list = []\n",
        "    for word in list(predicts_list.keys()):\n",
        "        #value = ((train_dict[word][1] / len(train_pos)) * (len(train_pos) / (len(train)))) / (train_dict[word][1]/len(train_pos)*(len(train_pos) / len(train)) + train_dict[word][0])/len(train_neg)*(len(train_neg) / len(train))\n",
        "        #value = float(train_dict[word][1]) / float(len(train) * (train_dict[word][1] + train_dict[word][0]))\n",
        "        value = (predicts_list[word][label] + lambda_value) / (predicts_list[word][1] + lambda_value + predicts_list[word][0] + lambda_value)\n",
        "        positive_list.append([word,value])\n",
        "    return positive_list\n",
        "positive_list = np.array(getTop10UsingDev(1))\n",
        "\n",
        "\n",
        "negative_list = np.array(getTop10UsingDev(0))\n",
        "\n",
        "\n",
        "print(\"Top 10 words that predicts  using dev data :\")\n",
        "printTop10(positive_list)\n",
        "print(\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipxL3LhzflUV"
      },
      "source": [
        "## f.Using the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "myFfQoAMflUW",
        "outputId": "d037314b-2b44-40a7-92c3-647e5806af4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy by using Smoothing of test dataset: 0.000%\n"
          ]
        }
      ],
      "source": [
        "print('The accuracy by using Smoothing of test dataset: %.3f%%' % accuracy_metric(data_test, smoothing_flag))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}